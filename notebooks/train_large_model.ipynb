{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "front-monkey",
   "metadata": {},
   "source": [
    "# Training the \"big\" model\n",
    "\n",
    "The code above will load training data and train the so-called \"big\" model.\n",
    "\n",
    "Let us first set up some parameters and load the train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "registered-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pndapetzim.data import load_datasets, LABEL_FILE_NAME, ORDER_FILE_NAME\n",
    "from pndapetzim.models import CategoricalFeatureDescriptor\n",
    "\n",
    "# Length of customer history to consider\n",
    "seq_len = 20\n",
    "\n",
    "# Relative weight of returning customer samples, an equivalent of oversampling.\n",
    "returning_weight = 5.0\n",
    "\n",
    "train, test, encodings = load_datasets(\n",
    "    order_path='../data/' + ORDER_FILE_NAME,\n",
    "    label_path='../data/' + LABEL_FILE_NAME,\n",
    "    seq_len=seq_len,\n",
    "    train_ratio=100,\n",
    "    returning_weight=returning_weight,\n",
    ")\n",
    "\n",
    "# This dictionary holds the number of unique values and the target size of the embedding space (hyperparameters).\n",
    "cat_features = {\n",
    "    'restaurant_id': CategoricalFeatureDescriptor(\n",
    "        vocab_size=encodings['restaurant_id'].vocab_size, embedding_size=15\n",
    "    ),\n",
    "    'city_id': CategoricalFeatureDescriptor(\n",
    "        vocab_size=encodings['city_id'].vocab_size, embedding_size=12\n",
    "    ),\n",
    "    'payment_id': CategoricalFeatureDescriptor(\n",
    "        vocab_size=encodings['payment_id'].vocab_size, embedding_size=3\n",
    "    ),\n",
    "    'platform_id': CategoricalFeatureDescriptor(\n",
    "        vocab_size=encodings['platform_id'].vocab_size, embedding_size=4\n",
    "    ),\n",
    "    'transmission_id': CategoricalFeatureDescriptor(\n",
    "        vocab_size=encodings['transmission_id'].vocab_size, embedding_size=4\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-booking",
   "metadata": {},
   "source": [
    "Now we can build model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "balanced-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from pndapetzim.models import build_large_model, CategoricalFeatureDescriptor\n",
    "\n",
    "lossm = BinaryCrossentropy()\n",
    "optimiser = Adam(learning_rate=0.01)\n",
    "\n",
    "model = build_large_model(seq_len, cat_features)\n",
    "aucm = AUC()\n",
    "recallm = Recall()\n",
    "metrics = ['accuracy', aucm, recallm]\n",
    "model.compile(loss=lossm, optimizer=optimiser, metrics=metrics)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-pattern",
   "metadata": {},
   "source": [
    "...and train it (takes some minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "owned-kidney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899/1899 [==============================] - 864s 453ms/step - loss: 1.0346 - accuracy: 0.6929 - auc: 0.7828 - recall: 0.7407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1032d2950>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.batch(batch_size).prefetch(10), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-headline",
   "metadata": {},
   "source": [
    "Finally, let us evaluate the trained model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "piano-imperial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 144ms/step - loss: 0.9574 - accuracy: 0.7428 - auc: 0.8113 - recall: 0.7068\n",
      "loss: 0.9574361443519592, accuracy: 0.7427983283996582, AUC: 0.8113062381744385, recall: 0.7067961096763611\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, auc, recall = model.evaluate(test.batch(batch_size))\n",
    "print(f'loss: {loss}, accuracy: {accuracy}, AUC: {auc}, recall: {recallm.result()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-sampling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
